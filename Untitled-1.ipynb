{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import loguniform\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import mpl_toolkits\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(file_name, isTrain):\n",
    "    train = pd.read_csv(file_name)\n",
    "    if isTrain:\n",
    "        # link in present only in train dataset\n",
    "        train = train.drop(['link'], axis=1)\n",
    "    fields = ['publication_date', 'latitude', 'longitude','category', 'floor', 'area', 'kupcha', 'rooms', 'repairs', 'hypothec', 'location attributes']\n",
    "    if isTrain:\n",
    "        fields.append('price')\n",
    "    train = train[fields]\n",
    "\n",
    "    # clean data to make it readable for the algorithm\n",
    "    train.replace('var', True, inplace=True)\n",
    "    train.replace('yoxdur', False, inplace=True)\n",
    "    train.replace(np.nan, False, inplace=True)\n",
    "    train.replace({'m²': ''}, regex=True, inplace=True)\n",
    "    train.replace('Yeni tikili', True, inplace=True)\n",
    "    train.replace('Köhnə tikili', False, inplace=True)\n",
    "    train = train.astype({\"area\": float})\n",
    "    currentFloors = []\n",
    "    maxFloors = []\n",
    "    for floor in train['floor'].array:\n",
    "        floors = floor.split(' / ')\n",
    "        currentFloors.append(floors[0])\n",
    "        maxFloors.append(floors[1])\n",
    "    train[\"current_floor\"] = currentFloors\n",
    "    train[\"max_floor\"] = maxFloors\n",
    "    \n",
    "    train = train.astype({\"current_floor\": int})\n",
    "    train = train.astype({\"max_floor\": int})\n",
    "    del train['floor']\n",
    "    \n",
    "    day = []\n",
    "    month = []\n",
    "    for index in range(0, len(train['publication_date'].array)):\n",
    "        element = train['publication_date'].array[index]\n",
    "        values = element.split(' ')\n",
    "        if values[1] == 'Yanvar':\n",
    "            month.append(1)\n",
    "        elif values[1] == 'Dekabr':\n",
    "            month.append(0)\n",
    "        day.append(values[0]) \n",
    "    del train['publication_date']\n",
    "    train['day'] = day\n",
    "    train['month'] = month\n",
    "\n",
    "    return train\n",
    "train = process('modified_train_binaaz.csv', True)\n",
    "tags = train[\"location attributes\"]\n",
    "clean_tags = tags.str.strip('[]\\'').str.split('\\', \\'')\n",
    "a = clean_tags.apply(pd.Series)\n",
    "b = a.stack()\n",
    "c = pd.get_dummies(b)\n",
    "d = c.groupby(level=0).sum()\n",
    "\n",
    "train = train.join(d)\n",
    "train = train.drop(['location attributes'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = process('test_binaaz_updated.csv', False)\n",
    "tags = test[\"location attributes\"]\n",
    "test_clean_tags = tags.str.strip('[]\\'').str.split('\\', \\'')\n",
    "a = test_clean_tags.apply(pd.Series)\n",
    "b = a.stack()\n",
    "c = pd.get_dummies(b)\n",
    "d = c.groupby(level=0).sum()\n",
    "\n",
    "tags = set()\n",
    "for ls in clean_tags:\n",
    "   for tag in ls:\n",
    "    tags.add(tag)\n",
    "\n",
    "test_tags = set()\n",
    "for ls in test_clean_tags:\n",
    "   for tag in ls:\n",
    "    test_tags.add(tag)\n",
    "\n",
    "to_add = tags.difference(test_tags)\n",
    "empty = []\n",
    "for i in range(0, len(d.index)):\n",
    "    empty.append(0)\n",
    "\n",
    "map = {'price': empty}\n",
    "for add in to_add:\n",
    "    map[add] = empty\n",
    "\n",
    "additional = pd.DataFrame(map)\n",
    "\n",
    "test = test.join(d)\n",
    "test = test.join(additional)\n",
    "to_drop = ['location attributes'] + list(test_tags.difference(tags))\n",
    "test = test.drop(to_drop, axis=1)\n",
    "\n",
    "target = 'price'\n",
    "columns = train.columns.tolist()\n",
    "columns = [c for c in columns if c not in [\"price\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 20 folds for each of 8 candidates, totalling 160 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>mean_test_error</th>\n",
       "      <th>std_test_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1300</td>\n",
       "      <td>6.968922e+09</td>\n",
       "      <td>4.525043e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1500</td>\n",
       "      <td>7.226502e+09</td>\n",
       "      <td>5.185937e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>900</td>\n",
       "      <td>7.380665e+09</td>\n",
       "      <td>4.800860e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1100</td>\n",
       "      <td>7.499963e+09</td>\n",
       "      <td>5.162184e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>700</td>\n",
       "      <td>7.596849e+09</td>\n",
       "      <td>4.466063e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500</td>\n",
       "      <td>7.964661e+09</td>\n",
       "      <td>4.750369e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>300</td>\n",
       "      <td>8.412495e+09</td>\n",
       "      <td>4.609775e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>1.016456e+10</td>\n",
       "      <td>5.189349e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  param_n_estimators  mean_test_error  std_test_error\n",
       "6               1300     6.968922e+09    4.525043e+09\n",
       "7               1500     7.226502e+09    5.185937e+09\n",
       "4                900     7.380665e+09    4.800860e+09\n",
       "5               1100     7.499963e+09    5.162184e+09\n",
       "3                700     7.596849e+09    4.466063e+09\n",
       "2                500     7.964661e+09    4.750369e+09\n",
       "1                300     8.412495e+09    4.609775e+09\n",
       "0                100     1.016456e+10    5.189349e+09"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_distributions = {\n",
    "    'n_estimators': range(100, 1501, 200),\n",
    "    # 2. 'max_depth': range(5, 71, 5),\n",
    "    # 2. 'min_samples_split': range(100, 1101, 200),\n",
    "    # 3. 'max_features': range(7, 20, 2),\n",
    "    # 4. 'subsample': [0.6,0.7,0.75,0.8,0.85,0.9]\n",
    "}\n",
    "\n",
    "\n",
    "gbr = GradientBoostingRegressor(learning_rate = 0.1, min_samples_split=500,\n",
    "                                 max_features='sqrt', subsample=0.8)\n",
    "search_cv = GridSearchCV(estimator=gbr, \n",
    "                   param_grid=param_distributions,\n",
    "                   scoring = 'neg_mean_squared_error', \n",
    "                   verbose=3,\n",
    "                   error_score='raise',\n",
    "                   n_jobs=-1,\n",
    "                   cv=20)\n",
    "\n",
    "search_cv.fit(train[columns], train[target])\n",
    "\n",
    "columns = [f\"param_{name}\" for name in param_distributions.keys()]\n",
    "columns += [\"mean_test_error\", \"std_test_error\"]\n",
    "cv_results = pd.DataFrame(search_cv.cv_results_)\n",
    "cv_results[\"mean_test_error\"] = -cv_results[\"mean_test_score\"]\n",
    "cv_results[\"std_test_error\"] = cv_results[\"std_test_score\"]\n",
    "cv_results[columns].sort_values(by=\"mean_test_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Eksklüziv təklif !\\nSəbail rayonu , Su İdman sarayının və Bulvarın yaxınlıqında , ekoloji baxımdan təmiz havası və sakitliyi baxımlndan çox yaxşı lokasiyada yerləşən , müasirliyi və klassikanı özündə birləşdirən Yeni tikili 6-mərtəbəli binanın 2-ci mərtəbəsində ümumi sahəsi 75kvm olan 3-otaqlı mənzil təcili satışa çıxdı.\\nMənzil SKAZNOYDUR.(Yelçəkən)\\nMənzil tam əla təmirlidir və əlavə təmirə ehtiyyac yoxdur.Mənzil 1-stuido mətbəx zal və 2-yataq otağından ibarətdir.(İstəyə uyğun olaraq mətbəxi ayırmaqda olar)\\nOtaqların hamısında pəncərə var və mənzil daim gün işıqı ilə təmin olunur.Otaqlar geniş , rahat və xoş auralıdır.\\nİstilik sistemi Kombi Kollektordur\\nSənəd Çıxarışdır\\nReal alıcı ilə qiymətdə razılaşmaq mümkündür.\\nQEYD-Aylıq stabil gəlir əldə etmək istəyənlər üçün unikal təklifdir.Mənzili alıb , tərəfimizdən rəsmi müqavilə əsasında aylıq 900-1000 kirayə verə bilərsiniz.'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m training \u001b[39m=\u001b[39m train\u001b[39m.\u001b[39msample(frac\u001b[39m=\u001b[39m\u001b[39m0.7\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[0;32m      3\u001b[0m test \u001b[39m=\u001b[39m train\u001b[39m.\u001b[39mloc[\u001b[39m~\u001b[39mtrain\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39misin(training\u001b[39m.\u001b[39mindex)]\n\u001b[1;32m----> 4\u001b[0m clf\u001b[39m.\u001b[39;49mfit(training[columns], training[target])\n\u001b[0;32m      5\u001b[0m clf \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict(test[columns])\n\u001b[0;32m      6\u001b[0m \u001b[39m# Compute error between our test predictions and the actual values.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Pavel\\Documents\\GitHub\\ufaz_ai\\.conda\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:429\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    423\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_clear_state()\n\u001b[0;32m    425\u001b[0m \u001b[39m# Check input\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[39m# Since check_array converts both X and y to the same dtype, but the\u001b[39;00m\n\u001b[0;32m    427\u001b[0m \u001b[39m# trees use different types for X and y, checking them separately.\u001b[39;00m\n\u001b[1;32m--> 429\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    430\u001b[0m     X, y, accept_sparse\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcoo\u001b[39;49m\u001b[39m\"\u001b[39;49m], dtype\u001b[39m=\u001b[39;49mDTYPE, multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m    431\u001b[0m )\n\u001b[0;32m    433\u001b[0m sample_weight_is_none \u001b[39m=\u001b[39m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    435\u001b[0m sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[1;32mc:\\Users\\Pavel\\Documents\\GitHub\\ufaz_ai\\.conda\\lib\\site-packages\\sklearn\\base.py:554\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    552\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[0;32m    553\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 554\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    555\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    557\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\Pavel\\Documents\\GitHub\\ufaz_ai\\.conda\\lib\\site-packages\\sklearn\\utils\\validation.py:1104\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1099\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1100\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1101\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1102\u001b[0m     )\n\u001b[1;32m-> 1104\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[0;32m   1105\u001b[0m     X,\n\u001b[0;32m   1106\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[0;32m   1107\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[0;32m   1108\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m   1109\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[0;32m   1110\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m   1111\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[0;32m   1112\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[0;32m   1113\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[0;32m   1114\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[0;32m   1115\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[0;32m   1116\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[0;32m   1117\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1118\u001b[0m )\n\u001b[0;32m   1120\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[0;32m   1122\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mc:\\Users\\Pavel\\Documents\\GitHub\\ufaz_ai\\.conda\\lib\\site-packages\\sklearn\\utils\\validation.py:808\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    803\u001b[0m \u001b[39mif\u001b[39;00m pandas_requires_conversion:\n\u001b[0;32m    804\u001b[0m     \u001b[39m# pandas dataframe requires conversion earlier to handle extension dtypes with\u001b[39;00m\n\u001b[0;32m    805\u001b[0m     \u001b[39m# nans\u001b[39;00m\n\u001b[0;32m    806\u001b[0m     \u001b[39m# Use the original dtype for conversion if dtype is None\u001b[39;00m\n\u001b[0;32m    807\u001b[0m     new_dtype \u001b[39m=\u001b[39m dtype_orig \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m dtype\n\u001b[1;32m--> 808\u001b[0m     array \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39;49mastype(new_dtype)\n\u001b[0;32m    809\u001b[0m     \u001b[39m# Since we converted here, we do not need to convert again later\u001b[39;00m\n\u001b[0;32m    810\u001b[0m     dtype \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Pavel\\Documents\\GitHub\\ufaz_ai\\.conda\\lib\\site-packages\\pandas\\core\\generic.py:6240\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m   6233\u001b[0m     results \u001b[39m=\u001b[39m [\n\u001b[0;32m   6234\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miloc[:, i]\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39mcopy)\n\u001b[0;32m   6235\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns))\n\u001b[0;32m   6236\u001b[0m     ]\n\u001b[0;32m   6238\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   6239\u001b[0m     \u001b[39m# else, only a single dtype is given\u001b[39;00m\n\u001b[1;32m-> 6240\u001b[0m     new_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mastype(dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m   6241\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(new_data)\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mastype\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   6243\u001b[0m \u001b[39m# GH 33113: handle empty frame or series\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Pavel\\Documents\\GitHub\\ufaz_ai\\.conda\\lib\\site-packages\\pandas\\core\\internals\\managers.py:450\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mastype\u001b[39m(\u001b[39mself\u001b[39m: T, dtype, copy: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, errors: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[1;32m--> 450\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply(\u001b[39m\"\u001b[39;49m\u001b[39mastype\u001b[39;49m\u001b[39m\"\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, errors\u001b[39m=\u001b[39;49merrors)\n",
      "File \u001b[1;32mc:\\Users\\Pavel\\Documents\\GitHub\\ufaz_ai\\.conda\\lib\\site-packages\\pandas\\core\\internals\\managers.py:352\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m         applied \u001b[39m=\u001b[39m b\u001b[39m.\u001b[39mapply(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    351\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 352\u001b[0m         applied \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(b, f)(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    353\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mNotImplementedError\u001b[39;00m):\n\u001b[0;32m    354\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m ignore_failures:\n",
      "File \u001b[1;32mc:\\Users\\Pavel\\Documents\\GitHub\\ufaz_ai\\.conda\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:526\u001b[0m, in \u001b[0;36mBlock.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    508\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[39mCoerce to the new dtype.\u001b[39;00m\n\u001b[0;32m    510\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[39mBlock\u001b[39;00m\n\u001b[0;32m    523\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    524\u001b[0m values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues\n\u001b[1;32m--> 526\u001b[0m new_values \u001b[39m=\u001b[39m astype_array_safe(values, dtype, copy\u001b[39m=\u001b[39;49mcopy, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[0;32m    528\u001b[0m new_values \u001b[39m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[0;32m    529\u001b[0m newb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_block(new_values)\n",
      "File \u001b[1;32mc:\\Users\\Pavel\\Documents\\GitHub\\ufaz_ai\\.conda\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py:299\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[1;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    296\u001b[0m     \u001b[39mreturn\u001b[39;00m values\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m    298\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 299\u001b[0m     new_values \u001b[39m=\u001b[39m astype_array(values, dtype, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[0;32m    300\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m):\n\u001b[0;32m    301\u001b[0m     \u001b[39m# e.g. astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[0;32m    302\u001b[0m     \u001b[39m#  trying to convert to float\u001b[39;00m\n\u001b[0;32m    303\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Pavel\\Documents\\GitHub\\ufaz_ai\\.conda\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py:230\u001b[0m, in \u001b[0;36mastype_array\u001b[1;34m(values, dtype, copy)\u001b[0m\n\u001b[0;32m    227\u001b[0m     values \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39mcopy)\n\u001b[0;32m    229\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 230\u001b[0m     values \u001b[39m=\u001b[39m astype_nansafe(values, dtype, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[0;32m    232\u001b[0m \u001b[39m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(dtype, np\u001b[39m.\u001b[39mdtype) \u001b[39mand\u001b[39;00m \u001b[39missubclass\u001b[39m(values\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mtype, \u001b[39mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Pavel\\Documents\\GitHub\\ufaz_ai\\.conda\\lib\\site-packages\\pandas\\core\\dtypes\\astype.py:170\u001b[0m, in \u001b[0;36mastype_nansafe\u001b[1;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[0;32m    168\u001b[0m \u001b[39mif\u001b[39;00m copy \u001b[39mor\u001b[39;00m is_object_dtype(arr\u001b[39m.\u001b[39mdtype) \u001b[39mor\u001b[39;00m is_object_dtype(dtype):\n\u001b[0;32m    169\u001b[0m     \u001b[39m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[1;32m--> 170\u001b[0m     \u001b[39mreturn\u001b[39;00m arr\u001b[39m.\u001b[39;49mastype(dtype, copy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    172\u001b[0m \u001b[39mreturn\u001b[39;00m arr\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39mcopy)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Eksklüziv təklif !\\nSəbail rayonu , Su İdman sarayının və Bulvarın yaxınlıqında , ekoloji baxımdan təmiz havası və sakitliyi baxımlndan çox yaxşı lokasiyada yerləşən , müasirliyi və klassikanı özündə birləşdirən Yeni tikili 6-mərtəbəli binanın 2-ci mərtəbəsində ümumi sahəsi 75kvm olan 3-otaqlı mənzil təcili satışa çıxdı.\\nMənzil SKAZNOYDUR.(Yelçəkən)\\nMənzil tam əla təmirlidir və əlavə təmirə ehtiyyac yoxdur.Mənzil 1-stuido mətbəx zal və 2-yataq otağından ibarətdir.(İstəyə uyğun olaraq mətbəxi ayırmaqda olar)\\nOtaqların hamısında pəncərə var və mənzil daim gün işıqı ilə təmin olunur.Otaqlar geniş , rahat və xoş auralıdır.\\nİstilik sistemi Kombi Kollektordur\\nSənəd Çıxarışdır\\nReal alıcı ilə qiymətdə razılaşmaq mümkündür.\\nQEYD-Aylıq stabil gəlir əldə etmək istəyənlər üçün unikal təklifdir.Mənzili alıb , tərəfimizdən rəsmi müqavilə əsasında aylıq 900-1000 kirayə verə bilərsiniz.'"
     ]
    }
   ],
   "source": [
    "\n",
    "clf = GradientBoostingRegressor(learning_rate=0.1, n_estimators = 1300, max_leaf_nodes= 50, min_samples_split=25, subsample=0.8, max_features=17, max_depth=50)\n",
    "training = train.sample(frac=0.7, random_state=5)\n",
    "test = train.loc[~train.index.isin(training.index)]\n",
    "clf.fit(training[columns], training[target])\n",
    "clf = clf.predict(test[columns])\n",
    "# Compute error between our test predictions and the actual values.\n",
    "lin_mse = mean_squared_error(clf, test[target], squared=False)\n",
    "print(\"Computed error:\", lin_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save the result\n",
    "result = pd.DataFrame(clf)\n",
    "result.to_csv(\"result.csv\", header=['price'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31e71d7ed0fa0b4c5d8ce44b530b41ec61e7cc3af5332ca9786b8048d514a256"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
